{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Scraper\n",
    "\n",
    "Below is our web scraper on Google Play built with the help of the Scrapy library. It works by first visting pages containing free top-selling apps for different categories and then extract their urls. Then it visits all app pages and extract the app name and description. When it is done it stores the data on file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from time import sleep\n",
    "from scrapy import signals\n",
    "from scrapy.xlib.pydispatch import dispatcher\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class GooglePlaySpider(scrapy.Spider):\n",
    "    name = 'googleplay_bot'\n",
    "    allowed_domains = ['play.google.com']\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.app_name_desc = {}\n",
    "\n",
    "        self.base_url = \"https://play.google.com\"\n",
    "        self.base_category_url = self.base_url + \"/store/apps/category/%s/collection/topselling_free\"\n",
    "        self.english_language_specifier_site = \"?hl=en\"\n",
    "        self.english_language_specifier_app = \"&hl=en\"\n",
    "\n",
    "        self.categories = [\"EDUCATION\", \"MEDICAL\", \"DATING\", \"COMMUNICATION\",\n",
    "                           \"FINANCE\", \"HEALTH_AND_FITNESS\",\n",
    "                           \"MUSIC_AND_AUDIO\", \"PERSONALIZATION\", \"ENTERTAINMENT\",\n",
    "                           \"EVENTS\", \"COMICS\", \"BEAUTY\", \"LIFESTYLE\", \"MAPS_AND_NAVIGATION\",\n",
    "                           \"HOUSE_AND_HOME\", \"BUSINESS\", \"BOOKS_AND_REFERENCES\", \"ART_AND_DESIGN\",\n",
    "                           \"FOOD_AND_DRINK\", \"ANDROID_WEAR\", \"GAME_ACTION\", \"GAME_ADVENTURE\"]\n",
    "\n",
    "        self.apps_visited = set()\n",
    "        self.max_apps = 1500\n",
    "        self.app_xpath = \"//a[@class='card-click-target']/@href\"\n",
    "\n",
    "        self.app_name_xpath = \"//div[@class='id-app-title']/text()\"\n",
    "        self.app_desc_xpath = \"//div[@itemprop='description']/div/text()\"\n",
    "\n",
    "        dispatcher.connect(self.spider_closed, signals.spider_closed)\n",
    "\n",
    "\n",
    "    def start_requests(self):\n",
    "        print(\"Start Requests\")\n",
    "\n",
    "        for category in self.categories:\n",
    "            url = self.base_category_url % category + self.english_language_specifier_site\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "        print(\"Parsing\", response.url)\n",
    "\n",
    "        app_urls = set(response.xpath(self.app_xpath).extract())\n",
    "\n",
    "        for app_url in app_urls:\n",
    "            url = self.base_url + app_url + self.english_language_specifier_app\n",
    "\n",
    "            if url not in self.apps_visited and len(self.apps_visited) < self.max_apps:\n",
    "                self.apps_visited.add(url)\n",
    "                yield scrapy.Request(url=url, callback=self.parse_app)\n",
    "\n",
    "    def parse_app(self, response):\n",
    "        print(\"Parsing app\", response.url)\n",
    "\n",
    "        name = \" \".join(response.xpath(self.app_name_xpath).extract())\n",
    "        desc = \" \".join(response.xpath(self.app_desc_xpath).extract())\n",
    "\n",
    "        self.app_name_desc[name] = desc\n",
    "\n",
    "    def spider_closed(self, spider):\n",
    "        with open(\"app_desc.json\", \"w\", encoding = \"utf-8\") as file:\n",
    "            json.dump(self.app_name_desc, file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    runner = CrawlerRunner()\n",
    "    d = runner.crawl(GooglePlaySpider)\n",
    "    d.addBoth(lambda _: reactor.stop())\n",
    "    reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "We built some helper functions to handle the pre-processing of the text for us. The pipeline follows the on proposed in the slides.\n",
    "\n",
    "1. Tokenization\n",
    "2. Removal of non-alphanumeric characters\n",
    "3. Lowercase\n",
    "4. Stopword removal\n",
    "5. Stemming\n",
    "\n",
    "Since the crawler saved the result on file, we just read the content and pre-process it, and again save it on file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tf_tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def rm_nonalphanum(words):\n",
    "    return [''.join(ch for ch in word if ch.isalnum()) for word in words]\n",
    "\n",
    "def tf_lowercase(words):\n",
    "    return [word.lower() for word in words]\n",
    "\n",
    "def rm_stopwords(words):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return [word for word in words if word not in stops]\n",
    "\n",
    "def tf_stem(words):\n",
    "    stemmer  = SnowballStemmer(\"english\")\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def tf_join(words):\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tf_process(text):\n",
    "    function_list =[tf_tokenize, rm_nonalphanum, tf_lowercase,\n",
    "                    rm_stopwords, tf_stem, tf_join]\n",
    "\n",
    "    for function in function_list:\n",
    "        text = function(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('app_desc.json', 'r', encoding=\"utf-8\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "processed_text = {}\n",
    "\n",
    "for name, desc in data.items():\n",
    "    processed_text[name] = tf_process(desc)\n",
    "\n",
    "\n",
    "with open(\"proc_app_desc.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(processed_text, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine\n",
    "\n",
    "When we have the data processed we can create our recommendation engine. We use the term-document inverse-document-frequency (tfidf) representation of all our data which is provided by scikit-learn package. So we read the processed data and train our model (TfidfVectorizer) which gives us a matrix representation of our data. The engine is then mostly finished, all we need to do is give it a query that gets processed the same way the data was originally processed and then transform it to a tfidf-vector using our trained model. Then we use the cosine similarity metric to find the n-most similar applications, where n = 10 in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLAY OF THE GAME MEME', 'Cartoon Photo Effect', 'Filters for Snapchat', 'Filters for Selfie', 'Filters for Musically', 'Wallpapers', 'Coloring Book For Adults Free ã€ColorWolfã€‘ ðŸº| ðŸ‡ºðŸ‡¸', 'ØªØ±ÙƒÙŠØ¨ Ø§Ù„ØµÙˆØ± ÙÙŠ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶', 'Textgram - write on photos', 'ComicScreen - ComicViewer']\n",
      "['HD Video Wallpapers', 'Masha and the Bear Child Games', 'Google Play Games', 'âˆž Infinity Loop', 'Twitch', 'Vimeo', 'App For Children - Kids games 1, 2, 3, 4 years old', 'Puzzle Magic - Games for kids 1-5 years old', 'Exploration Lite', 'FunToysReview']\n",
      "['PayPal', 'Send Money Globally WU', 'WorldRemit Money Transfer', 'Gimi - a piggy bank', 'Nordea Mobilbanken FÃ¶retag', 'Dreams - Save to your dreams, easily & for free.', 'Nordea Mobile Bank â€“ Sweden', 'Seqr - mobile payments', 'Ferratum Mobile Bank', 'Swish payments']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open('proc_app_desc.json', 'r', encoding=\"utf-8\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "descriptions = list(data.values())\n",
    "app_names = list(data.keys())\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_desc_mat = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "def get_recommendations(query, n=10):\n",
    "    query = [tf_process(query)]\n",
    "    tfidf_query_vec = vectorizer.transform(query)\n",
    "\n",
    "    search_result = cosine_similarity(tfidf_query_vec, tfidf_desc_mat).flatten().tolist()\n",
    "    index = np.argsort(search_result, axis = 0)[::-1][:n]\n",
    "    recommendations = [app_names[i] for i in index]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "print(get_recommendations(\"photo image social\"))\n",
    "print(get_recommendations(\"game video fun\"))\n",
    "print(get_recommendations(\"money debt loan\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see that the recommendations seem appropriate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
