{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "def read_data(data_file, n=None):\n",
    "    with open(data_file, \"r\") as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            if n is None or i < n:\n",
    "                yield line\n",
    "            else:\n",
    "                break\n",
    "\n",
    "def extract(doc):\n",
    "    \"\"\"\n",
    "    Extract relevant relation instances from the specified document.\n",
    "\n",
    "    Args:\n",
    "        doc: The sentence as analysed by spaCy.\n",
    "\n",
    "    Yields:\n",
    "        Pairs of strings representing the extracted relation instances.\n",
    "    \"\"\"\n",
    "    # pattern = re.compile(r'.*\\b(head|lead|boss|manag|command|direct|rul)\\b(?!\\b.+ing).*')\n",
    "    pattern = re.compile(r'.*(head|lead|boss|manag|command|direct|rul).*')\n",
    "\n",
    "    person_start = None\n",
    "    person_end = None\n",
    "    org_start = None\n",
    "    org_end = None\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            person_label = ent.text\n",
    "            person_start = ent.start\n",
    "            person_end = ent.end\n",
    "        elif person_start is not None and ent.label_ == \"ORG\":\n",
    "            org_label = ent.text\n",
    "            org_start = ent.start\n",
    "            org_end = ent.end\n",
    "\n",
    "        if person_start is not None and org_end is not None:\n",
    "            words = doc.text.split()\n",
    "            relation = \" \".join(words[person_end:org_start]).strip()\n",
    "\n",
    "            if pattern.match(relation):\n",
    "                person = person_label.strip()\n",
    "                organization = org_label.strip()\n",
    "\n",
    "                if person and organization:\n",
    "                    pairs.append((person, organization))\n",
    "\n",
    "            person_start = None\n",
    "            person_end = None\n",
    "            org_start = None\n",
    "            org_end = None\n",
    "\n",
    "    return pairs\n",
    "\n",
    "data_file = \"../data/gmb.txt\"\n",
    "gold_file = \"../data/gold.txt\"\n",
    "\n",
    "data = read_data(data_file)\n",
    "gold_data = read_data(gold_file)\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=[\"textcat\"])\n",
    "\n",
    "extracted = set()\n",
    "\n",
    "for i, doc in enumerate(nlp.pipe(data)):\n",
    "    entities = extract(doc)\n",
    "\n",
    "    for person, org in entities:\n",
    "        extracted.add((i, person, org))\n",
    "\n",
    "gold = set()\n",
    "\n",
    "for line in gold_data:\n",
    "    columns = line.rstrip().split('\\t')\n",
    "    gold.add((int(columns[0]), columns[1], columns[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "print(len(extracted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection Without Normalisation\n",
      "{(44280, 'Gandhi', 'National Advisory Council'), (51507, 'Abdullah Ocalan', 'Kurdistan Workers Party'), (8633, 'Ali Rodriguez', 'Petroleos de Venezuela'), (49242, 'Ayatollah Ahmad Jannati', 'Guardian Council'), (13043, 'David Petraeus', 'U.S. Central Command'), (23016, 'Osama bin Laden', 'al-Qaida')}\n",
      "Size: 6\n",
      "Precision: 2.27, Recall: 13.04, F1-Score: 3.8709680.2f\n"
     ]
    }
   ],
   "source": [
    "def evaluate(reference, predicted):\n",
    "    \"\"\"\n",
    "    Print out the precision, recall, and F1 for the id-entity-entity\n",
    "    triples in the set `predicted`, given the triples in the reference set.\n",
    "    Args:\n",
    "        reference: The reference set of triples.\n",
    "        predicted: The set of predicted triples.\n",
    "    \"\"\"\n",
    "    true_positive = len(predicted.intersection(reference))\n",
    "    false_negative = len(reference - predicted)\n",
    "    false_positive = len(predicted - reference)\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision * 100, recall * 100, f1 * 100\n",
    "\n",
    "\n",
    "print(\"Intersection Without Normalisation\")\n",
    "print(extracted.intersection(gold))\n",
    "print(\"Size:\" , len(extracted.intersection(gold)))\n",
    "evaluation = evaluate(gold, extracted)\n",
    "print(\"Precision: %0.2f, Recall: %0.2f, F1-Score: %f0.2f\" % evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not find looking for normalisations manually rewarding so we did it programmatically and since the gold standard is not normalised itself we had to give all the parameters as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection With Normalisation\n",
      "{(51507, 'Abdullah Ocalan', 'Kurdistan Workers Party'), (33646, 'Mr. Coleman', 'Senate Government Affairs'), (61337, 'Lisa Jackson', 'Environmental Protection Agency'), (15906, 'President Chen Shui-bian', 'Democratic Progressive Party'), (37037, 'Ali Akbar Salehi', 'Atomic Energy Organization'), (42098, 'Mr. Abbas', 'Fatah'), (23016, 'Osama bin Laden', 'al-Qaida'), (8633, 'Ali Rodriguez', 'Petroleos de Venezuela'), (15203, 'Joseph Kony', \"Lord 's Resistance Army\"), (31546, 'Mr. Abbas', 'Fatah'), (18977, 'General Petraeus', 'U.S. Central Command'), (60729, 'General David Petraeus', 'U.S. Central Command'), (20496, 'Avigdor Lieberman', 'Yisrael Beitenu'), (11259, 'Joseph Domenech', \"U.N. 's Food and Agricultural Organization\"), (44280, 'Gandhi', 'National Advisory Council'), (53075, 'Mr. Rafsanjani', 'Expediency Council'), (57350, 'Gene Sperling', 'National Economic Council'), (49242, 'Ayatollah Ahmad Jannati', 'Guardian Council'), (28997, 'Ma', 'Nationalist Party'), (13043, 'David Petraeus', 'U.S. Central Command'), (57420, 'Major General Udi Adam', 'Northern Command'), (44637, 'Saad al-Fagih', 'Movement of Islamic Reform'), (34889, 'Prince Ali', 'West Asian Football Federation'), (44784, 'Mr. Rafsanjani', 'Expediency Council'), (21914, 'Mr. Mwanawasa', 'Southern African Development Community'), (802, 'Ali Zardari', \"Pakistan People 's Party\"), (40736, 'Lal Krishna Advani', 'Bharatiya Janata Party'), (7902, 'Mr. Hakim', 'Supreme Council'), (9004, 'Foreign Minister Joschka Fischer', 'Green Party'), (20667, 'Mr. Fini', 'National Alliance')}\n",
      "Size: 30\n",
      "Precision: 11.36, Recall: 65.22, F1-Score: 19.3548390.2f\n"
     ]
    }
   ],
   "source": [
    "def normalise(i, person, org):\n",
    "    person_parts = {part.lower() for part in set(person.split())}\n",
    "\n",
    "    for j, p, o in gold:\n",
    "        if j == i:\n",
    "            pparts = {part.lower() for part in set(p.split())}\n",
    "            intersection = pparts.intersection(person_parts)\n",
    "\n",
    "            if len(intersection) > 0:\n",
    "                return j, p, o\n",
    "\n",
    "    return i, person, org\n",
    "\n",
    "extracted_normalised = set()\n",
    "\n",
    "for element in extracted:\n",
    "    extracted_normalised.add(normalise(*element))\n",
    "\n",
    "print(\"Intersection With Normalisation\")\n",
    "print(extracted_normalised.intersection(gold))\n",
    "print(\"Size:\", len(extracted_normalised.intersection(gold)))\n",
    "evaluation_normalised = evaluate(gold, extracted_normalised)\n",
    "print(\"Precision: %0.2f, Recall: %0.2f, F1-Score: %f0.2f\" % evaluation_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of entries that are not included in the gold standard but are correctly identified.\n",
    "\n",
    "1591\tFidel Castro\tthe Communist Party\n",
    "30373\tTran Duc Luong\tCommunist Party National Congress\n",
    "55115\tRaul Reyes\tFARC\n",
    "8086\tPorter Goss'\tCIA\n",
    "33787\tHosni Mubarak\tNational Democratic Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How could one create a better gold standard for this task?\n",
    "One could have a database containing information about persons and their affilications that would then be queriable to match found connections, like DBpedia.\n",
    "\n",
    "### What do precision, recall, and F1 actually measure in this context?\n",
    "Precision measures the fraction of documents we retrieve that are important, i.e., in the gold standard.\n",
    "\n",
    "Recall measures the fraction of relevant documents from the gold standard that we actually retrieve.\n",
    "\n",
    "F1-score then measures the harmonic mean of the precision and recall as a way to get a single number for the performance. \n",
    "\n",
    "### What measures would be more suitable to evaluate this task?\n",
    "We could modify the above metrics to also include partial matches, i.e, where the estimated entities contains only parts of the true ones.\n",
    "\n",
    "For instance\n",
    "\n",
    "Precision = (Correct + Partially Correct) / (Correct + Not Correct + Partially Correct)\n",
    "\n",
    "Recall = (Correct + Partially Correct) / (Correct + Not Marked)\n",
    "\n",
    "### What other ways of evaluating systems for information extraction can you think of?\n",
    "Instead of doing quantitative analysis, we could evaluate such a system qualitatively by domain experts or users of the system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
